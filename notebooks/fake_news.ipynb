{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow import keras\n",
    "\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evz.ro</td>\n",
       "      <td>Ministrul spulberă informațiile despre pensii....</td>\n",
       "      <td>Ministrul Muncii Violeta Alexandru a declarat ...</td>\n",
       "      <td>https://evz.ro/bomba-despre-pensii-ministrul-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adevarul.ro</td>\n",
       "      <td>G4Media: Klaus Iohannis i-a cerut premierului ...</td>\n",
       "      <td>Preşedintele Klaus Iohannis i-a cerut, vineri,...</td>\n",
       "      <td>https://adevarul.ro/news/politica/g4media-klau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>puterea.ro</td>\n",
       "      <td>ULTIMA ORĂ: Medicul Mircea Beuran a fost REȚIN...</td>\n",
       "      <td>Medicul Mircea Beuran a fost reținut pentru 24...</td>\n",
       "      <td>https://www.puterea.ro/eveniment/ultima-ora-me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agerpres.ro</td>\n",
       "      <td>Iohannis, în Israel: România - angajată să con...</td>\n",
       "      <td>\\r\\nTrimisul special al AGERPRES, Florentina P...</td>\n",
       "      <td>https://www.agerpres.ro/politica/2020/01/21/io...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news.ro</td>\n",
       "      <td>Avertizări cod galben şi cod portocaliu de plo...</td>\n",
       "      <td>Meteorologii au emis saâmbătă seară avertizări...</td>\n",
       "      <td>https://www.news.ro/social/avertizari-cod-galb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                                              title  \\\n",
       "0       evz.ro  Ministrul spulberă informațiile despre pensii....   \n",
       "1  adevarul.ro  G4Media: Klaus Iohannis i-a cerut premierului ...   \n",
       "2   puterea.ro  ULTIMA ORĂ: Medicul Mircea Beuran a fost REȚIN...   \n",
       "3  agerpres.ro  Iohannis, în Israel: România - angajată să con...   \n",
       "4      news.ro  Avertizări cod galben şi cod portocaliu de plo...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Ministrul Muncii Violeta Alexandru a declarat ...   \n",
       "1  Preşedintele Klaus Iohannis i-a cerut, vineri,...   \n",
       "2  Medicul Mircea Beuran a fost reținut pentru 24...   \n",
       "3  \\r\\nTrimisul special al AGERPRES, Florentina P...   \n",
       "4  Meteorologii au emis saâmbătă seară avertizări...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://evz.ro/bomba-despre-pensii-ministrul-s...  \n",
       "1  https://adevarul.ro/news/politica/g4media-klau...  \n",
       "2  https://www.puterea.ro/eveniment/ultima-ora-me...  \n",
       "3  https://www.agerpres.ro/politica/2020/01/21/io...  \n",
       "4  https://www.news.ro/social/avertizari-cod-galb...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/ro_news.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = df['source'].apply(lambda x: x in ('puterea.ro', 'b1.ro')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import RomanianStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(txt):\n",
    "    tokens = word_tokenize(txt)\n",
    "    stemmer = RomanianStemmer()\n",
    "    # remove all tokens that are not alphabetic\n",
    "    words = [stemmer.stem(word.lower()) for word in tokens if word.isalpha()]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['title_clean'] = df['title'].apply(clean)\n",
    "df['text_clean'] = df['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(X):\n",
    "    chars_ = set()\n",
    "    for sent in X:\n",
    "        for w in sent:\n",
    "            for c in w:\n",
    "                chars_.add(c)\n",
    "    chars_ = sorted(list(chars_))\n",
    "    char2idx = { c : i + 1 for i, c in enumerate(chars_)}\n",
    "    return char2idx\n",
    "\n",
    "def get_char_features(X, char2idx, sent_size=10, char_feat_size=10):\n",
    "    def word2charidxs(word, char2idx):\n",
    "        char_feats = list(map(lambda c : char2idx.get(c, 0), word))\n",
    "        return char_feats\n",
    "    \n",
    "    X_chars = []\n",
    "    for sent in X:\n",
    "        sent_indx = list(map(lambda x: word2charidxs(x, char2idx), sent))\n",
    "        sent_indx = keras.preprocessing.sequence.pad_sequences(maxlen=sent_size,\n",
    "            sequences=sent_indx, padding=\"post\", truncating=\"post\", value=0)\n",
    "        X_chars.append(sent_indx)\n",
    "    pad_val = np.zeros((sent_size, char_feat_size))\n",
    "    X_chars = keras.preprocessing.sequence.pad_sequences(maxlen=sent_size, sequences=X_chars,\n",
    "        padding=\"post\", truncating=\"post\", value = pad_val)\n",
    "    return X_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "title_char2idx = get_vocabulary(df['title_clean'])\n",
    "text_char2idx = get_vocabulary(df['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_title = get_char_features(df['title_clean'], title_char2idx)\n",
    "X_text = get_char_features(df['text_clean'], text_char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char encoding\n",
    "TITLE_SENT_SIZE = 10\n",
    "TEXT_SENT_SIZE = 100\n",
    "CHAR_FEAT_SIZE = 10\n",
    "\n",
    "def title_only_model(char2idx):\n",
    "    # TITLE INPUT\n",
    "    title_input = keras.layers.Input(shape=(TITLE_SENT_SIZE, CHAR_FEAT_SIZE))\n",
    "    title_char_emb = keras.layers.TimeDistributed(keras.layers.Embedding(input_dim=len(char2idx) + 1,\n",
    "        output_dim=30, input_length=CHAR_FEAT_SIZE))(title_input)  \n",
    "\n",
    "    title_char_dropout = keras.layers.Dropout(0.5)(title_char_emb)\n",
    "    title_char_conv1d = keras.layers.TimeDistributed(keras.layers.Conv1D(kernel_size=3, filters=32,\n",
    "        padding='same',activation='tanh', strides=1))(title_char_dropout)\n",
    "    title_char_maxpool = keras.layers.TimeDistributed(keras.layers.MaxPooling1D(CHAR_FEAT_SIZE))(title_char_conv1d)\n",
    "    title_char_feats = keras.layers.TimeDistributed(keras.layers.Flatten())(title_char_maxpool)\n",
    "\n",
    "    #all_feat = keras.layers.concatenate([char_feats])\n",
    "    all_feat = title_char_feats\n",
    "\n",
    "    all_out = keras.layers.SpatialDropout1D(0.3)(all_feat)\n",
    "\n",
    "    bi_lstm = keras.layers.Bidirectional(keras.layers.LSTM(units=100,\n",
    "            return_sequences=False))(all_out)\n",
    "\n",
    "    out = keras.layers.Dense(1, activation=\"sigmoid\")(bi_lstm)\n",
    "\n",
    "    model = keras.models.Model([title_input], out)\n",
    "    model.compile(optimizer='adam', loss=keras.losses.binary_crossentropy, metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def title_and_text_model(title_char2idx, text_char2idx):\n",
    "    # TITLE INPUT\n",
    "    title_input = keras.layers.Input(shape=(TITLE_SENT_SIZE, CHAR_FEAT_SIZE))\n",
    "    title_char_emb = keras.layers.TimeDistributed(keras.layers.Embedding(input_dim=len(title_char2idx) + 1,\n",
    "        output_dim=30, input_length=CHAR_FEAT_SIZE))(title_input)  \n",
    "\n",
    "    title_char_dropout = keras.layers.Dropout(0.5)(title_char_emb)\n",
    "    title_char_conv1d = keras.layers.TimeDistributed(keras.layers.Conv1D(kernel_size=3, filters=32,\n",
    "        padding='same',activation='tanh', strides=1))(title_char_dropout)\n",
    "    title_char_maxpool = keras.layers.TimeDistributed(keras.layers.MaxPooling1D(CHAR_FEAT_SIZE))(title_char_conv1d)\n",
    "    title_char_feats = keras.layers.TimeDistributed(keras.layers.Flatten())(title_char_maxpool)\n",
    "    \n",
    "    # TEXT INPUT\n",
    "    text_input = keras.layers.Input(shape=(TEXT_SENT_SIZE, CHAR_FEAT_SIZE))\n",
    "    text_char_emb = keras.layers.TimeDistributed(keras.layers.Embedding(input_dim=len(text_char2idx) + 1,\n",
    "        output_dim=30, input_length=CHAR_FEAT_SIZE))(text_input)  \n",
    "\n",
    "    text_char_dropout = keras.layers.Dropout(0.5)(text_char_emb)\n",
    "    text_char_conv1d = keras.layers.TimeDistributed(keras.layers.Conv1D(kernel_size=3, filters=32,\n",
    "        padding='same',activation='tanh', strides=1))(text_char_dropout)\n",
    "    text_char_maxpool = keras.layers.TimeDistributed(keras.layers.MaxPooling1D(CHAR_FEAT_SIZE))(text_char_conv1d)\n",
    "    text_char_feats = keras.layers.TimeDistributed(keras.layers.Flatten())(text_char_maxpool)\n",
    "    \n",
    "    all_feat = keras.layers.concatenate([title_char_feats, text_char_feats])\n",
    "\n",
    "    all_out = keras.layers.SpatialDropout1D(0.3)(all_feat)\n",
    "\n",
    "    bi_lstm = keras.layers.Bidirectional(keras.layers.LSTM(units=100,\n",
    "            return_sequences=False))(all_out)\n",
    "\n",
    "    out = keras.layers.Dense(1, activation=\"sigmoid\")(bi_lstm)\n",
    "\n",
    "    model = keras.models.Model([title_input, text_input], out)\n",
    "    model.compile(optimizer='adam', loss=keras.losses.binary_crossentropy, metrics=['acc'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_model = title_only_model(title_char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17663 samples, validate on 1963 samples\n",
      "Epoch 1/10\n",
      "17663/17663 [==============================] - 9s 503us/sample - loss: 0.4094 - acc: 0.8576 - val_loss: 0.3936 - val_acc: 0.8630\n",
      "Epoch 2/10\n",
      "17663/17663 [==============================] - 6s 315us/sample - loss: 0.3952 - acc: 0.8607 - val_loss: 0.3977 - val_acc: 0.8630\n",
      "Epoch 3/10\n",
      "17663/17663 [==============================] - 6s 314us/sample - loss: 0.3895 - acc: 0.8613 - val_loss: 0.3897 - val_acc: 0.8645\n",
      "Epoch 4/10\n",
      "17663/17663 [==============================] - 6s 314us/sample - loss: 0.3885 - acc: 0.8621 - val_loss: 0.3875 - val_acc: 0.8635\n",
      "Epoch 5/10\n",
      "17663/17663 [==============================] - 6s 314us/sample - loss: 0.3858 - acc: 0.8621 - val_loss: 0.3948 - val_acc: 0.8625\n",
      "Epoch 6/10\n",
      "17663/17663 [==============================] - 6s 317us/sample - loss: 0.3847 - acc: 0.8626 - val_loss: 0.3958 - val_acc: 0.8645\n",
      "Epoch 7/10\n",
      "17663/17663 [==============================] - 6s 318us/sample - loss: 0.3824 - acc: 0.8627 - val_loss: 0.3800 - val_acc: 0.8650\n",
      "Epoch 8/10\n",
      "17663/17663 [==============================] - 6s 318us/sample - loss: 0.3815 - acc: 0.8625 - val_loss: 0.3820 - val_acc: 0.8650\n",
      "Epoch 9/10\n",
      "17663/17663 [==============================] - 6s 318us/sample - loss: 0.3813 - acc: 0.8624 - val_loss: 0.3919 - val_acc: 0.8625\n",
      "Epoch 10/10\n",
      "17663/17663 [==============================] - 6s 318us/sample - loss: 0.3794 - acc: 0.8628 - val_loss: 0.4043 - val_acc: 0.8543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f984d8ec48>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_model.fit(x=X_title, y=y, batch_size=32, epochs=10, validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = title_and_text_model(title_char2idx, text_char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17663 samples, validate on 1963 samples\n",
      "Epoch 1/10\n",
      "17663/17663 [==============================] - 10s 582us/sample - loss: 0.3656 - acc: 0.8770 - val_loss: 0.3448 - val_acc: 0.8874\n",
      "Epoch 2/10\n",
      "17663/17663 [==============================] - 7s 384us/sample - loss: 0.3356 - acc: 0.8882 - val_loss: 0.3358 - val_acc: 0.8874\n",
      "Epoch 3/10\n",
      "17663/17663 [==============================] - 7s 387us/sample - loss: 0.3302 - acc: 0.8884 - val_loss: 0.3328 - val_acc: 0.8879\n",
      "Epoch 4/10\n",
      "17663/17663 [==============================] - 7s 385us/sample - loss: 0.3276 - acc: 0.8887 - val_loss: 0.3314 - val_acc: 0.8895\n",
      "Epoch 5/10\n",
      "17663/17663 [==============================] - 7s 388us/sample - loss: 0.3206 - acc: 0.8906 - val_loss: 0.3466 - val_acc: 0.8833\n",
      "Epoch 6/10\n",
      "17663/17663 [==============================] - 7s 387us/sample - loss: 0.3170 - acc: 0.8910 - val_loss: 0.3217 - val_acc: 0.8889\n",
      "Epoch 7/10\n",
      "17663/17663 [==============================] - 7s 390us/sample - loss: 0.3094 - acc: 0.8912 - val_loss: 0.3251 - val_acc: 0.8900\n",
      "Epoch 8/10\n",
      "17663/17663 [==============================] - 7s 390us/sample - loss: 0.3091 - acc: 0.8911 - val_loss: 0.3185 - val_acc: 0.8900\n",
      "Epoch 9/10\n",
      "17663/17663 [==============================] - 7s 387us/sample - loss: 0.3076 - acc: 0.8909 - val_loss: 0.3197 - val_acc: 0.8895\n",
      "Epoch 10/10\n",
      "17663/17663 [==============================] - 7s 387us/sample - loss: 0.3024 - acc: 0.8915 - val_loss: 0.3247 - val_acc: 0.8889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f9882e1c08>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.fit(x=[X_title, X_text], y=y, batch_size=32, epochs=10, validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('char2idxs.pkl', 'wb') as f:\n",
    "    pickle.dump({'text_char2idx' : text_char2idx,\n",
    "        'title_char2idx' : title_char2idx,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.save_weights('../model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow20",
   "language": "python",
   "name": "tensorflow20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
